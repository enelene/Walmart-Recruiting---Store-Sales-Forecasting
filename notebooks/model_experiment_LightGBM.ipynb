{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6705dc94-ca4c-4295-a8ef-ff075763aa8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fbTS-TUsgP0",
    "outputId": "3bcf32ef-c75a-4cfc-8b40-1221814b7e7c"
   },
   "outputs": [],
   "source": [
    "%pip install lightgbm mlflow optuna scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "770f336f-6d3e-4f18-b4a0-375fc7358c69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import optuna\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from src.preprocessing import advanced_feature_engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d989bf72-be5a-4196-8efd-c9307d7f1da6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# LOAD PROCESSED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b922b2da-bc4e-4b42-b6d1-77bd93f00d78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PROCESSED_DIR = '/dbfs/FileStore/walmart_project/data/processed'\n",
    "TRAIN_PATH = os.path.join(PROCESSED_DIR, 'train_processed_final.csv')\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv(TRAIN_PATH)\n",
    "    print(\"Successfully loaded processed training data from DBFS.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Processed data not found at '{TRAIN_PATH}'.\")\n",
    "    dbutils.notebook.exit(\"Data preparation notebook must be run first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2743736-593a-4037-99a3-6e4e4c82564c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MLFLOW SETUP FOR DAGSHUB AND MODEL PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3896b25-7a10-40d4-a7f3-ed8ad6f0ce8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install dagshub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fb14fc4-b765-408c-9748-fc7ba6c9b141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dagshub\n",
    "dagshub.init(repo_owner='enelene', repo_name='Walmart-Recruiting---Store-Sales-Forecasting', mlflow=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2385c0e-0a0c-4e98-8402-7cfbd2b30827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"https://dagshub.com/enelene/Walmart-Recruiting---Store-Sales-Forecasting.mlflow\")\n",
    "\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'enelene'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = 'cbe8109dbe80931664d754dbd476356414fa62a0'\n",
    "\n",
    "EXPERIMENT_NAME = \"LightGBM_Training\"\n",
    "print(f\"Experiment set to: '{EXPERIMENT_NAME}'\")\n",
    "\n",
    "\n",
    "# Define Features (X), Target (y), and the WMAE metric\n",
    "TARGET = 'Weekly_Sales'\n",
    "features_to_drop = [TARGET, 'Date']\n",
    "categorical_features = ['Store', 'Dept', 'IsHoliday', 'Year', 'Month', 'WeekOfYear', 'HasMarkdown', 'Store_Dept', 'Type_A', 'Type_B', 'Type_C']\n",
    "features = [col for col in train_df.columns if col not in features_to_drop]\n",
    "# Convert categorical features for LightGBM\n",
    "for col in categorical_features:\n",
    "    if col in train_df.columns:\n",
    "        train_df[col] = train_df[col].astype('category')\n",
    "X = train_df[features]\n",
    "y = train_df[TARGET]\n",
    "\n",
    "def wmae(y_true, y_pred, is_holiday):\n",
    "    weights = np.where(is_holiday, 5, 1)\n",
    "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "994db3ba-5895-48d9-9be9-40809e3d4fe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MLFLOW EXPERIMENT RUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74aa97be-d4af-4ef7-832b-a5c0178038d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"LGBM_Baseline\"):\n",
    "    print(\"\\n--- Starting Run: LGBM_Baseline ---\")\n",
    "    model = lgb.LGBMRegressor(random_state=42)\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    wmae_scores = []\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        X_t, X_v = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_t, y_v = y.iloc[train_index], y.iloc[val_index]\n",
    "        model.fit(X_t, y_t)\n",
    "        preds = model.predict(X_v)\n",
    "        score = wmae(y_v, preds, X_v['IsHoliday'].astype(bool))\n",
    "        wmae_scores.append(score)\n",
    "    avg_wmae = np.mean(wmae_scores)\n",
    "    print(f\"Baseline Average WMAE: {avg_wmae:.2f}\")\n",
    "    mlflow.log_metric(\"avg_wmae_cv\", avg_wmae)\n",
    "\n",
    "# == Run 2: Hyperparameter Tuning with Optuna ==\n",
    "with mlflow.start_run(run_name=\"LGBM_Hyperparameter_Tuning\"):\n",
    "    print(\"\\n--- Starting Run: LGBM_Hyperparameter_Tuning ---\")\n",
    "    train_idx, val_idx = list(tscv.split(X))[-1]\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'objective': 'regression_l1', 'metric': 'mae', 'n_estimators': 1000,\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "            'verbose': -1, 'n_jobs': -1, 'seed': 42\n",
    "        }\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
    "                  eval_metric='mae', callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "        preds = model.predict(X_val)\n",
    "        return wmae(y_val, preds, X_val['IsHoliday'].astype(bool))\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    best_params = study.best_params\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"best_wmae_tuned\", study.best_value)\n",
    "\n",
    "# == Run 3: Final Model & Registration ==\n",
    "with mlflow.start_run(run_name=\"LGBM_Final_Pipeline\"):\n",
    "    print(\"\\n--- Starting Run: LGBM_Final_Pipeline ---\")\n",
    "    final_params = best_params\n",
    "    final_params['n_estimators'] = 2000\n",
    "    final_params['random_state'] = 42\n",
    "    mlflow.log_params(final_params)\n",
    "\n",
    "    final_model = lgb.LGBMRegressor(**final_params)\n",
    "    print(\"Training final model on all data...\")\n",
    "    final_model.fit(X, y)\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    class WalmartSalesPipeline(mlflow.pyfunc.PythonModel):\n",
    "        def __init__(self, model, feature_engineering_fn, training_columns):\n",
    "            self.model = model\n",
    "            self._feature_engineering_fn = feature_engineering_fn\n",
    "            self._training_columns = training_columns\n",
    "        \n",
    "        def predict(self, context, model_input):\n",
    "            processed_input = self._feature_engineering_fn(model_input)\n",
    "            processed_input = processed_input.reindex(columns=self._training_columns, fill_value=0)\n",
    "            return self.model.predict(processed_input)\n",
    "\n",
    "    print(\"Logging and registering the final model pipeline to Dagshub...\")\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"lightgbm-full-pipeline\",\n",
    "        python_model=WalmartSalesPipeline(final_model, advanced_feature_engineering, features),\n",
    "        registered_model_name=\"LightGBM-Walmart-Sales-Pipeline\",\n",
    "        input_example=X.head(5)\n",
    "    )\n",
    "    print(\"Model Pipeline successfully logged and registered to Dagshub MLflow!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ec109c5-9527-470b-a177-785af863a721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2nd Iteration with some advanced featueres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "467fe85f-beef-406c-8426-d0697cda2e77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"LGBM_Baseline_V2\"):\n",
    "    print(\"\\n--- Starting Run: LGBM_Baseline_V2 ---\")\n",
    "    model = lgb.LGBMRegressor(random_state=42)\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    wmae_scores = []\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        X_t, X_v = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_t, y_v = y.iloc[train_index], y.iloc[val_index]\n",
    "        # Tell LightGBM which features are categorical\n",
    "        model.fit(X_t, y_t, categorical_feature=[col for col in categorical_features if col in X_t.columns])\n",
    "        preds = model.predict(X_v)\n",
    "        score = wmae(y_v, preds, X_v['IsHoliday'].astype(bool))\n",
    "        wmae_scores.append(score)\n",
    "    avg_wmae = np.mean(wmae_scores)\n",
    "    print(f\"Baseline Average WMAE (V2 Features): {avg_wmae:.2f}\")\n",
    "    mlflow.log_metric(\"avg_wmae_cv\", avg_wmae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc0d6c65-8de3-4ee0-92fb-dfbac1834505",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# == Run 2: Hyperparameter Tuning with Optuna ==\n",
    "with mlflow.start_run(run_name=\"LGBM_Hyperparameter_Tuning_V2\"):\n",
    "    print(\"\\n--- Starting Run: LGBM_Hyperparameter_Tuning_V2 ---\")\n",
    "    train_idx, val_idx = list(tscv.split(X))[-1]\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'objective': 'regression_l1', 'metric': 'mae', 'n_estimators': 1000,\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "            'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "            'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "            'verbose': -1, 'n_jobs': -1, 'seed': 42\n",
    "        }\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        model.fit(X_train, y_train, \n",
    "                  eval_set=[(X_val, y_val)],\n",
    "                  eval_metric='mae', \n",
    "                  callbacks=[lgb.early_stopping(50, verbose=False)],\n",
    "                  categorical_feature=[col for col in categorical_features if col in X_train.columns])\n",
    "        preds = model.predict(X_val)\n",
    "        return wmae(y_val, preds, X_val['IsHoliday'].astype(bool))\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=25) # Increased trials slightly\n",
    "    best_params = study.best_params\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"best_wmae_tuned\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c1972f4-9ddd-4838-85be-cd32d0c40655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# == Run 3: Final Model & Registration ==\n",
    "with mlflow.start_run(run_name=\"LGBM_Final_Pipeline_V2\"):\n",
    "    print(\"\\n--- Starting Run: LGBM_Final_Pipeline_V2 ---\")\n",
    "    final_params = best_params\n",
    "    final_params['n_estimators'] = 2500 # Increased estimators\n",
    "    final_params['random_state'] = 42\n",
    "    mlflow.log_params(final_params)\n",
    "\n",
    "    final_model = lgb.LGBMRegressor(**final_params)\n",
    "    print(\"Training final model on all data...\")\n",
    "    final_model.fit(X, y, categorical_feature=[col for col in categorical_features if col in X.columns])\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    class WalmartSalesPipeline(mlflow.pyfunc.PythonModel):\n",
    "        def __init__(self, model, feature_engineering_fn, training_columns):\n",
    "            self.model = model\n",
    "            self._feature_engineering_fn = feature_engineering_fn\n",
    "            self._training_columns = training_columns\n",
    "        \n",
    "        def predict(self, context, model_input):\n",
    "            processed_input = self._feature_engineering_fn(model_input)\n",
    "            \n",
    "            # Convert categorical features for the model\n",
    "            for col in self._training_columns:\n",
    "                if processed_input[col].dtype.name == 'category':\n",
    "                    processed_input[col] = processed_input[col].astype('category')\n",
    "\n",
    "            processed_input = processed_input.reindex(columns=self._training_columns, fill_value=0)\n",
    "            return self.model.predict(processed_input)\n",
    "\n",
    "    raw_train_df = pd.read_csv('/dbfs/FileStore/walmart_project/data/raw/train.csv')\n",
    "    \n",
    "    print(\"Logging and registering the final model pipeline to Dagshub...\")\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"lightgbm-full-pipeline-v2\",\n",
    "        python_model=WalmartSalesPipeline(final_model, advanced_feature_engineering, features),\n",
    "        registered_model_name=\"LightGBM-Walmart-Sales-Pipeline\",\n",
    "        input_example=raw_train_df.head(5) # Use raw data for the example\n",
    "    )\n",
    "    print(\"Model Pipeline successfully logged and registered to Dagshub MLflow!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "model_experiment_LightGBM",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
