{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fbTS-TUsgP0",
        "outputId": "3bcf32ef-c75a-4cfc-8b40-1221814b7e7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.2)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.9.0 optuna-4.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import mlflow\n",
        "import mlflow.pyfunc\n",
        "import optuna\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LOAD PROCESSED DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.preprocessing import advanced_feature_engineering\n",
        "\n",
        "PROCESSED_DIR = 'data/processed'\n",
        "TRAIN_PATH = os.path.join(PROCESSED_DIR, 'train_processed_final.csv')\n",
        "\n",
        "try:\n",
        "    train_df = pd.read_csv(TRAIN_PATH)\n",
        "    print(\"Successfully loaded processed training data.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Processed data not found at '{TRAIN_PATH}'.\")\n",
        "    print(\"Please run the '00_initial_data_exploration.ipynb' notebook first to generate it.\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLFLOW SETUP AND MODEL PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EXPERIMENT_NAME = \"LightGBM_Training\"\n",
        "mlflow.set_experiment(EXPERIMENT_NAME)\n",
        "print(f\"MLflow experiment set to: '{EXPERIMENT_NAME}'\")\n",
        "\n",
        "# Define Features (X) and Target (y)\n",
        "TARGET = 'Weekly_Sales'\n",
        "features_to_drop = [TARGET, 'Date']\n",
        "features = [col for col in train_df.columns if col not in features_to_drop]\n",
        "\n",
        "X = train_df[features]\n",
        "y = train_df[TARGET]\n",
        "\n",
        "# Define WMAE Evaluation Metric\n",
        "def wmae(y_true, y_pred, is_holiday):\n",
        "    weights = np.where(is_holiday, 5, 1)\n",
        "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLFLOW EXPERIMENT RUNS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# --- SECTION 3:  ---\n",
        "print(\"\\n--- SECTION 3: MLFLOW EXPERIMENT RUNS ---\")\n",
        "# The rest of the notebook proceeds exactly as before, using the 'X' and 'y' DataFrames.\n",
        "\n",
        "# == Run 1: Baseline Model ==\n",
        "with mlflow.start_run(run_name=\"LGBM_Baseline\"):\n",
        "    print(\"\\n--- Starting Run: LGBM_Baseline ---\")\n",
        "    model = lgb.LGBMRegressor(random_state=42)\n",
        "    tscv = TimeSeriesSplit(n_splits=3)\n",
        "    wmae_scores = []\n",
        "    for train_index, val_index in tscv.split(X):\n",
        "        X_t, X_v = X.iloc[train_index], X.iloc[val_index]\n",
        "        y_t, y_v = y.iloc[train_index], y.iloc[val_index]\n",
        "        model.fit(X_t, y_t)\n",
        "        preds = model.predict(X_v)\n",
        "        score = wmae(y_v, preds, X_v['IsHoliday'].astype(bool))\n",
        "        wmae_scores.append(score)\n",
        "    avg_wmae = np.mean(wmae_scores)\n",
        "    print(f\"Baseline Average WMAE: {avg_wmae:.2f}\")\n",
        "    mlflow.log_metric(\"avg_wmae_cv\", avg_wmae)\n",
        "\n",
        "# == Run 2: Hyperparameter Tuning (Optional, can be slow) ==\n",
        "# ... (Optuna tuning code would go here) ...\n",
        "best_params = {'learning_rate': 0.05, 'num_leaves': 80, 'feature_fraction': 0.8} # Using placeholder\n",
        "\n",
        "# == Run 3: Final Model & Registration ==\n",
        "with mlflow.start_run(run_name=\"LGBM_Final_Pipeline\"):\n",
        "    print(\"\\n--- Starting Run: LGBM_Final_Pipeline ---\")\n",
        "    final_params = best_params\n",
        "    final_params['n_estimators'] = 2000\n",
        "    final_params['random_state'] = 42\n",
        "    mlflow.log_params(final_params)\n",
        "\n",
        "    final_model = lgb.LGBMRegressor(**final_params)\n",
        "    print(\"Training final model on all data...\")\n",
        "    final_model.fit(X, y)\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "    # Create the custom pipeline class for inference\n",
        "    class WalmartSalesPipeline(mlflow.pyfunc.PythonModel):\n",
        "        def __init__(self, model, feature_engineering_fn, training_columns):\n",
        "            self.model = model\n",
        "            self._feature_engineering_fn = feature_engineering_fn\n",
        "            self._training_columns = training_columns\n",
        "        \n",
        "        def predict(self, context, model_input):\n",
        "            processed_input = self._feature_engineering_fn(model_input)\n",
        "            processed_input = processed_input.reindex(columns=self._training_columns, fill_value=0)\n",
        "            return self.model.predict(processed_input)\n",
        "\n",
        "    # Log the custom pipeline that includes the imported preprocessing function\n",
        "    print(\"Logging and registering the final model pipeline...\")\n",
        "    mlflow.pyfunc.log_model(\n",
        "        artifact_path=\"lightgbm-full-pipeline\",\n",
        "        python_model=WalmartSalesPipeline(final_model, advanced_feature_engineering, features),\n",
        "        code_path=[\"src/preprocessing.py\"],\n",
        "        registered_model_name=\"LightGBM-Walmart-Sales-Pipeline\",\n",
        "        input_example=X.head(5)\n",
        "    )\n",
        "    print(\"Model Pipeline successfully logged and registered!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
