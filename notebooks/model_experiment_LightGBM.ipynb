{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Sw5FMegJsWlp",
        "outputId": "ff65ba84-681e-42ef-e53d-00bba8cd579c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-3.1.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting mlflow-skinny==3.1.1 (from mlflow)\n",
            "  Downloading mlflow_skinny-3.1.1-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.2)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<21,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.3)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.41)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.1.1->mlflow)\n",
            "  Downloading databricks_sdk-0.57.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.115.13)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (8.7.0)\n",
            "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.1.1->mlflow)\n",
            "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.1.1->mlflow)\n",
            "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (24.2)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (2.11.7)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (4.14.0)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.34.3)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.4.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==3.1.1->mlflow) (0.46.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.1->mlflow) (3.23.0)\n",
            "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow)\n",
            "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (2025.6.15)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==3.1.1->mlflow) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (4.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.6.1)\n",
            "Downloading mlflow-3.1.1-py3-none-any.whl (24.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-3.1.1-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.57.0-py3-none-any.whl (733 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m733.8/733.8 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gunicorn, graphql-core, opentelemetry-api, graphql-relay, docker, alembic, opentelemetry-semantic-conventions, graphene, databricks-sdk, opentelemetry-sdk, mlflow-skinny, mlflow\n",
            "Successfully installed alembic-1.16.2 databricks-sdk-0.57.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-3.1.1 mlflow-skinny-3.1.1 opentelemetry-api-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fbTS-TUsgP0",
        "outputId": "3bcf32ef-c75a-4cfc-8b40-1221814b7e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.2)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.9.0 optuna-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import zipfile\n",
        "import os\n",
        "import optuna # For hyperparameter tuning\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n"
      ],
      "metadata": {
        "id": "VBmNFn7Ss26K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvkfAmf6sIpN",
        "outputId": "c5fd9c12-e0b5-429b-e0e7-c0d1f49c82b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLflow experiment set to: 'LightGBM_Training'\n",
            "Loading raw data files...\n",
            "ERROR: Raw data files not found in 'data/raw'. Please run the master notebook first.\n"
          ]
        }
      ],
      "source": [
        "# 2.1: MLflow Setup\n",
        "EXPERIMENT_NAME = \"LightGBM_Training\"\n",
        "mlflow.set_experiment(EXPERIMENT_NAME)\n",
        "print(f\"MLflow experiment set to: '{EXPERIMENT_NAME}'\")\n",
        "\n",
        "# 2.2: Load Raw Merged Data\n",
        "# The pipeline will handle all preprocessing, so we start with the raw merged data.\n",
        "try:\n",
        "    # We re-create the merged data here to ensure this notebook is self-contained\n",
        "    # In a real project, this might be a function imported from a script.\n",
        "    print(\"Loading raw data files...\")\n",
        "    with zipfile.ZipFile('data/raw/train.csv.zip', 'r') as z:\n",
        "        train_df = pd.read_csv(z.open('train.csv'))\n",
        "    with zipfile.ZipFile('data/raw/features.csv.zip', 'r') as z:\n",
        "        features_df = pd.read_csv(z.open('features.csv'))\n",
        "    stores_df = pd.read_csv('data/raw/stores.csv')\n",
        "\n",
        "    raw_train_data = train_df.merge(features_df, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "    raw_train_data = raw_train_data.merge(stores_df, on='Store', how='left')\n",
        "    print(\"Raw training data loaded and merged.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: Raw data files not found in 'data/raw'. Please run the master notebook first.\")\n",
        "    exit()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.3: Define Custom Evaluation Metric\n",
        "# Weighted Mean Absolute Error (WMAE) as per Kaggle competition\n",
        "def wmae(y_true, y_pred, is_holiday):\n",
        "    weights = np.where(is_holiday, 5, 1)\n",
        "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "\n",
        "# --- SECTION 3: PREPROCESSING PIPELINE DEFINITION ---\n",
        "print(\"\\n--- SECTION 3: PREPROCESSING PIPELINE DEFINITION ---\")\n",
        "# This is the core of our reproducible model. This function will be part of the pipeline.\n",
        "\n",
        "def feature_engineering_transformer(df):\n",
        "    \"\"\"\n",
        "    Function to be wrapped in a scikit-learn FunctionTransformer.\n",
        "    Mirrors the logic from the master notebook.\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "    df_copy['Date'] = pd.to_datetime(df_copy['Date'])\n",
        "    if 'Weekly_Sales' in df_copy.columns:\n",
        "        df_copy['Weekly_Sales'] = df_copy['Weekly_Sales'].clip(lower=0)\n",
        "\n",
        "    # Holiday Features\n",
        "    thanksgiving_dates = pd.to_datetime([\"2010-11-26\", \"2011-11-25\"])\n",
        "    super_bowl_dates = pd.to_datetime([\"2010-02-12\", \"2011-02-11\", \"2012-02-10\"])\n",
        "    labor_day_dates = pd.to_datetime([\"2010-09-10\", \"2011-09-09\", \"2012-09-07\"])\n",
        "    christmas_dates = pd.to_datetime([\"2010-12-31\", \"2011-12-30\"])\n",
        "    df_copy['IsBlackFridayWeek'] = df_copy.Date.isin(thanksgiving_dates).astype(int)\n",
        "    df_copy['IsSuperBowlWeek'] = df_copy.Date.isin(super_bowl_dates).astype(int)\n",
        "\n",
        "    # Time-Based Features\n",
        "    df_copy['Year'] = df_copy['Date'].dt.year\n",
        "    df_copy['Month'] = df_copy['Date'].dt.month\n",
        "    df_copy['WeekOfYear'] = df_copy['Date'].dt.isocalendar().week.astype(int)\n",
        "    df_copy['DayOfWeek'] = df_copy['Date'].dt.dayofweek\n",
        "\n",
        "    # Lag Features (requires Weekly_Sales)\n",
        "    df_copy.sort_values(by=['Store', 'Dept', 'Date'], inplace=True)\n",
        "    if 'Weekly_Sales' in df_copy.columns:\n",
        "        df_copy['Sales_Lag_1'] = df_copy.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1)\n",
        "        df_copy['Sales_Lag_52'] = df_copy.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(52)\n",
        "\n",
        "    # Missing Value Handling\n",
        "    df_copy[['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']] = df_copy[['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']].fillna(0)\n",
        "    df_copy.fillna(method='ffill', inplace=True)\n",
        "    df_copy.fillna(method='bfill', inplace=True)\n",
        "    df_copy.fillna(0, inplace=True)\n",
        "\n",
        "    # One-Hot Encoding for 'Type'\n",
        "    df_copy = pd.get_dummies(df_copy, columns=['Type'], prefix='Type')\n",
        "\n",
        "    # Define and select final features\n",
        "    final_features = [\n",
        "        'Store', 'Dept', 'IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Size',\n",
        "        'IsBlackFridayWeek', 'IsSuperBowlWeek', 'Year', 'Month', 'WeekOfYear', 'DayOfWeek',\n",
        "        'Sales_Lag_1', 'Sales_Lag_52', 'Type_A', 'Type_B', 'Type_C'\n",
        "    ]\n",
        "    # Ensure all feature columns exist, fill with 0 if not (can happen in test set)\n",
        "    for col in final_features:\n",
        "        if col not in df_copy.columns:\n",
        "            df_copy[col] = 0\n",
        "\n",
        "    return df_copy[final_features]\n",
        "\n",
        "# Wrap the function in a scikit-learn compatible transformer\n",
        "preprocessor = FunctionTransformer(feature_engineering_transformer)\n",
        "\n",
        "# --- SECTION 4: MLFLOW EXPERIMENT RUNS ---\n",
        "print(\"\\n--- SECTION 4: MLFLOW EXPERIMENT RUNS ---\")\n",
        "\n",
        "# Prepare data for modeling\n",
        "X = raw_train_data.drop('Weekly_Sales', axis=1)\n",
        "y = raw_train_data['Weekly_Sales']\n",
        "\n",
        "# == Run 1: Baseline Model ==\n",
        "with mlflow.start_run(run_name=\"LGBM_Baseline\"):\n",
        "    print(\"\\n--- Starting Run: LGBM_Baseline ---\")\n",
        "    mlflow.log_param(\"model_type\", \"LightGBM\")\n",
        "    mlflow.log_param(\"tuning\", \"default_hyperparameters\")\n",
        "\n",
        "    # Create the full pipeline\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', lgb.LGBMRegressor(random_state=42))\n",
        "    ])\n",
        "\n",
        "    # Time series cross-validation\n",
        "    tscv = TimeSeriesSplit(n_splits=3)\n",
        "    wmae_scores = []\n",
        "\n",
        "    for train_index, val_index in tscv.split(X):\n",
        "        X_t, X_v = X.iloc[train_index], X.iloc[val_index]\n",
        "        y_t, y_v = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "        # Fit the pipeline on the training fold\n",
        "        pipeline.fit(X_t.assign(Weekly_Sales=y_t), y_t)\n",
        "\n",
        "        # Predict on the validation fold\n",
        "        preds = pipeline.predict(X_v.assign(Weekly_Sales=y_v))\n",
        "\n",
        "        # We need the 'IsHoliday' column from the original validation set for WMAE\n",
        "        is_holiday_val = X_v['IsHoliday'].astype(bool)\n",
        "        score = wmae(y_v, preds, is_holiday_val)\n",
        "        wmae_scores.append(score)\n",
        "\n",
        "    avg_wmae = np.mean(wmae_scores)\n",
        "    print(f\"Baseline Average WMAE: {avg_wmae:.2f}\")\n",
        "    mlflow.log_metric(\"avg_wmae_cv\", avg_wmae)\n",
        "\n",
        "# == Run 2: Hyperparameter Tuning with Optuna ==\n",
        "with mlflow.start_run(run_name=\"LGBM_Hyperparameter_Tuning\"):\n",
        "    print(\"\\n--- Starting Run: LGBM_Hyperparameter_Tuning ---\")\n",
        "\n",
        "    # Use a single, fixed split for faster tuning\n",
        "    train_idx, val_idx = list(tscv.split(X))[-1]\n",
        "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "    # Preprocess the data once before the tuning loop\n",
        "    X_train_processed = preprocessor.fit_transform(X_train.assign(Weekly_Sales=y_train))\n",
        "    X_val_processed = preprocessor.transform(X_val.assign(Weekly_Sales=y_val))\n",
        "    is_holiday_val = X_val['IsHoliday'].astype(bool)\n",
        "\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            'objective': 'regression_l1',\n",
        "            'metric': 'mae',\n",
        "            'n_estimators': 1000,\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
        "            'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
        "            'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "            'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
        "            'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
        "            'verbose': -1,\n",
        "            'n_jobs': -1,\n",
        "            'seed': 42\n",
        "        }\n",
        "\n",
        "        model = lgb.LGBMRegressor(**params)\n",
        "        model.fit(X_train_processed, y_train,\n",
        "                  eval_set=[(X_val_processed, y_val)],\n",
        "                  eval_metric='mae',\n",
        "                  callbacks=[lgb.early_stopping(50, verbose=False)])\n",
        "\n",
        "        preds = model.predict(X_val_processed)\n",
        "        score = wmae(y_val, preds, is_holiday_val)\n",
        "        return score\n",
        "\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(objective, n_trials=20) # Increase n_trials for better results\n",
        "\n",
        "    print(f\"Best WMAE from tuning: {study.best_value:.2f}\")\n",
        "    mlflow.log_params(study.best_params)\n",
        "    mlflow.log_metric(\"best_wmae_tuned\", study.best_value)\n",
        "\n",
        "# == Run 3: Final Model Pipeline & Registration ==\n",
        "with mlflow.start_run(run_name=\"LGBM_Final_Pipeline\"):\n",
        "    print(\"\\n--- Starting Run: LGBM_Final_Pipeline ---\")\n",
        "\n",
        "    # Get best params from the tuning run\n",
        "    best_params = study.best_params\n",
        "    best_params['n_estimators'] = 2000 # Train on full data with more estimators\n",
        "    best_params['random_state'] = 42\n",
        "    mlflow.log_params(best_params)\n",
        "\n",
        "    # Create the final pipeline with the best hyperparameters\n",
        "    final_pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', lgb.LGBMRegressor(**best_params))\n",
        "    ])\n",
        "\n",
        "    # Train the final pipeline on the ENTIRE dataset\n",
        "    print(\"Training final pipeline on all data...\")\n",
        "    final_pipeline.fit(X.assign(Weekly_Sales=y), y)\n",
        "    print(\"Final pipeline training complete.\")\n",
        "\n",
        "    # Log the final pipeline to MLflow and register it\n",
        "    print(\"Logging and registering the final model...\")\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=final_pipeline,\n",
        "        artifact_path=\"lightgbm-pipeline\",\n",
        "        registered_model_name=\"LightGBM-Walmart-Sales\",\n",
        "        input_example=X.head(5) # Provide an input example for schema inference\n",
        "    )\n",
        "    print(\"Model successfully logged and registered!\")\n"
      ],
      "metadata": {
        "id": "ve3zV9zBtB3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "IkNv82AJsySU",
        "outputId": "45e9feb7-e3c0-4654-9e8c-cdd8853d8a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1252343134.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
          ]
        }
      ]
    }
  ]
}