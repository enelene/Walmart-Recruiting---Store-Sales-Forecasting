{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4AoeO9kpVLQ"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY5U7uKk-RXz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import zipfile\n",
        "import getpass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ygyq96Sso89c"
      },
      "source": [
        " # DOWNLOAD AND UNZIP RAW DATA "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOea0xr183pj",
        "outputId": "5c7702b8-91e4-490c-9dd6-a58dfc0f73c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kaggle API authenticated successfully.\n",
            "Data files already exist. Skipping download.\n"
          ]
        }
      ],
      "source": [
        "DATA_DIR = 'data/raw'\n",
        "COMPETITION_NAME = 'walmart-recruiting-store-sales-forecasting'\n",
        "\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    os.makedirs(DATA_DIR)\n",
        "\n",
        "if not os.path.exists(os.path.join(DATA_DIR, 'train.csv')):\n",
        "    print(\"Raw data not found. Attempting to download from Kaggle...\")\n",
        "    \n",
        "    # Universal Authentication Block\n",
        "    try:\n",
        "        # This works on a local machine where kaggle.json is in ~/.kaggle/\n",
        "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "        api = KaggleApi()\n",
        "        api.authenticate()\n",
        "        print(\"Kaggle API authenticated successfully (Local).\")\n",
        "    except (OSError, ImportError):\n",
        "        # This block will be triggered in Google Colab\n",
        "        print(\"Local authentication failed. Attempting Colab setup...\")\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            # Prompt user to upload kaggle.json\n",
        "            if not os.path.exists('kaggle.json'):\n",
        "                 print(\"Please upload your kaggle.json file.\")\n",
        "                 files.upload()\n",
        "            \n",
        "            # Create Kaggle directory and move the file\n",
        "            if not os.path.exists(os.path.expanduser('~/.kaggle')):\n",
        "                os.makedirs(os.path.expanduser('~/.kaggle'))\n",
        "            os.rename('kaggle.json', os.path.expanduser('~/.kaggle/kaggle.json'))\n",
        "            os.chmod(os.path.expanduser('~/.kaggle/kaggle.json'), 0o600)\n",
        "            \n",
        "            from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "            api = KaggleApi()\n",
        "            api.authenticate()\n",
        "            print(\"Kaggle API authenticated successfully (Colab).\")\n",
        "        except Exception as e:\n",
        "            print(f\"Colab authentication failed. Error: {e}\")\n",
        "            api = None\n",
        "\n",
        "    # Download and Unzip if authentication was successful\n",
        "    if api:\n",
        "        print(f\"Downloading data for competition '{COMPETITION_NAME}'...\")\n",
        "        api.competition_download_files(COMPETITION_NAME, path=DATA_DIR, quiet=True)\n",
        "        \n",
        "        master_zip_path = os.path.join(DATA_DIR, f'{COMPETITION_NAME}.zip')\n",
        "        with zipfile.ZipFile(master_zip_path, 'r') as z:\n",
        "            z.extractall(DATA_DIR)\n",
        "        for item in ['train.csv.zip', 'test.csv.zip', 'features.csv.zip']:\n",
        "            with zipfile.ZipFile(os.path.join(DATA_DIR, item), 'r') as z:\n",
        "                z.extractall(DATA_DIR)\n",
        "        print(\"Data successfully downloaded and unzipped.\")\n",
        "    else:\n",
        "        print(\"FATAL: Could not authenticate with Kaggle. Cannot proceed.\")\n",
        "else:\n",
        "    print(\"Raw data already exists. Skipping download.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LOAD, MERGE, AND PROCESS DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from src.preprocessing import advanced_feature_engineering\n",
        "except ImportError:\n",
        "    print(\"ERROR: Could not import 'advanced_feature_engineering'.\")\n",
        "    print(\"Please ensure you are running this notebook from your project's root directory and that 'src/preprocessing.py' exists.\")\n",
        "    exit()\n",
        "\n",
        "# Load raw data from the 'data/raw' folder\n",
        "train_df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n",
        "features_df = pd.read_csv(os.path.join(DATA_DIR, 'features.csv'))\n",
        "stores_df = pd.read_csv(os.path.join(DATA_DIR, 'stores.csv'))\n",
        "\n",
        "# Merge data\n",
        "raw_train_data = train_df.merge(features_df, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "raw_train_data = raw_train_data.merge(stores_df, on='Store', how='left')\n",
        "\n",
        "# Apply the advanced feature engineering\n",
        "print(\"Applying feature engineering...\")\n",
        "train_processed = advanced_feature_engineering(raw_train_data)\n",
        "print(\"Processing complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SAVE PROCESSED DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROCESSED_DIR = 'data/processed'\n",
        "if not os.path.exists(PROCESSED_DIR):\n",
        "    os.makedirs(PROCESSED_DIR)\n",
        "\n",
        "# Save the train set\n",
        "train_processed.to_csv(os.path.join(PROCESSED_DIR, 'train_processed_final.csv'), index=False)\n",
        "\n",
        "# Process and save the test set\n",
        "test_df = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))\n",
        "raw_test_data = test_df.merge(features_df, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "raw_test_data = raw_test_data.merge(stores_df, on='Store', how='left')\n",
        "# Add a dummy 'Weekly_Sales' column for the feature engineering function to work\n",
        "raw_test_data['Weekly_Sales'] = 0 \n",
        "test_processed = advanced_feature_engineering(raw_test_data)\n",
        "test_processed.drop('Weekly_Sales', axis=1, inplace=True) # Drop the dummy column\n",
        "test_processed.to_csv(os.path.join(PROCESSED_DIR, 'test_processed_final.csv'), index=False)\n",
        "\n",
        "print(f\"Final processed datasets have been saved to the '{PROCESSED_DIR}' directory.\")\n",
        "print(\"You are now ready to run your modeling notebooks.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
