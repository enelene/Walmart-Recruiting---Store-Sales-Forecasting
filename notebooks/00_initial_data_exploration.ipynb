{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a1acb69-d5fd-4d1a-a7ba-54f7222df19f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "K4AoeO9kpVLQ"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "884eeb78-9a64-4e98-a8b8-a5e260ec209a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51cc80d8-d268-4986-a33e-5404ea391b2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "fY5U7uKk-RXz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55be223b-4f6f-4155-9131-ca0821bedf26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Ygyq96Sso89c"
   },
   "source": [
    "--- SECTION 1: KAGGLE SETUP FOR DATABRICKS ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5da9b1e-b804-498b-9058-dcf924d99e07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tOea0xr183pj",
    "outputId": "5c7702b8-91e4-490c-9dd6-a58dfc0f73c5"
   },
   "outputs": [],
   "source": [
    "os.environ['KAGGLE_USERNAME'] = \"elenegabeskiria\"\n",
    "os.environ['KAGGLE_KEY'] = \"fbc7c735b9a28fa8d6fe48b75ebe1d6b\"\n",
    "\n",
    "DATA_DIR = '/dbfs/FileStore/walmart_project/data/raw'\n",
    "COMPETITION_NAME = 'walmart-recruiting-store-sales-forecasting'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(os.path.join(DATA_DIR, 'train.csv')):\n",
    "    print(\"Raw data not found. Downloading from Kaggle...\")\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    \n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    \n",
    "    api.competition_download_files(COMPETITION_NAME, path=DATA_DIR, quiet=True)\n",
    "\n",
    "    master_zip_path = os.path.join(DATA_DIR, f'{COMPETITION_NAME}.zip')\n",
    "    with zipfile.ZipFile(master_zip_path, 'r') as z:\n",
    "        z.extractall(DATA_DIR)\n",
    "    for item in ['train.csv.zip', 'test.csv.zip', 'features.csv.zip']:\n",
    "        with zipfile.ZipFile(os.path.join(DATA_DIR, item), 'r') as z:\n",
    "            z.extractall(DATA_DIR)\n",
    "    print(\"Data successfully downloaded and unzipped to DBFS.\")\n",
    "else:\n",
    "    print(\"Raw data already exists in DBFS. Skipping download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d79a3e5-50a6-4cec-bc01-4027398076b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# LOAD, MERGE, AND PROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57108599-091c-4876-a37a-c21ad1c0a9ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from src.preprocessing import advanced_feature_engineering\n",
    "\n",
    "# Load raw data from DBFS\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n",
    "features_df = pd.read_csv(os.path.join(DATA_DIR, 'features.csv'))\n",
    "stores_df = pd.read_csv(os.path.join(DATA_DIR, 'stores.csv'))\n",
    "\n",
    "# Merge data\n",
    "raw_train_data = train_df.merge(features_df, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
    "raw_train_data = raw_train_data.merge(stores_df, on='Store', how='left')\n",
    "\n",
    "# Apply the advanced feature engineering\n",
    "print(\"Applying feature engineering...\")\n",
    "train_processed = advanced_feature_engineering(raw_train_data)\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "117a5272-f959-489f-a461-3193bea1346c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SAVE PROCESSED DATA TO DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bd3304f-5cde-4161-9ab4-9b8bf63ab585",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PROCESSED_DIR = '/dbfs/FileStore/walmart_project/data/processed'\n",
    "if not os.path.exists(PROCESSED_DIR):\n",
    "    os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# Save the train set\n",
    "train_processed.to_csv(os.path.join(PROCESSED_DIR, 'train_processed_final.csv'), index=False)\n",
    "\n",
    "# Process and save the test set\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))\n",
    "raw_test_data = test_df.merge(features_df, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
    "raw_test_data = raw_test_data.merge(stores_df, on='Store', how='left')\n",
    "test_processed = advanced_feature_engineering(raw_test_data)\n",
    "test_processed.to_csv(os.path.join(PROCESSED_DIR, 'test_processed_final.csv'), index=False)\n",
    "\n",
    "print(f\"Final processed datasets have been saved to the '{PROCESSED_DIR}' directory in DBFS.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_initial_data_exploration",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
