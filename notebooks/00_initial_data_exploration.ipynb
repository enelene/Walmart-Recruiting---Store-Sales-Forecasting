{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a1acb69-d5fd-4d1a-a7ba-54f7222df19f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "K4AoeO9kpVLQ"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "884eeb78-9a64-4e98-a8b8-a5e260ec209a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51cc80d8-d268-4986-a33e-5404ea391b2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "fY5U7uKk-RXz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55be223b-4f6f-4155-9131-ca0821bedf26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Ygyq96Sso89c"
   },
   "source": [
    "--- SECTION 1: KAGGLE SETUP FOR DATABRICKS ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5da9b1e-b804-498b-9058-dcf924d99e07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tOea0xr183pj",
    "outputId": "5c7702b8-91e4-490c-9dd6-a58dfc0f73c5"
   },
   "outputs": [],
   "source": [
    "os.environ['KAGGLE_USERNAME'] = \"elenegabeskiria\"\n",
    "os.environ['KAGGLE_KEY'] = \"fbc7c735b9a28fa8d6fe48b75ebe1d6b\"\n",
    "\n",
    "DATA_DIR = '/dbfs/FileStore/walmart_project/data/raw'\n",
    "COMPETITION_NAME = 'walmart-recruiting-store-sales-forecasting'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(os.path.join(DATA_DIR, 'train.csv')):\n",
    "    print(\"Raw data not found. Downloading from Kaggle...\")\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    \n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    \n",
    "    api.competition_download_files(COMPETITION_NAME, path=DATA_DIR, quiet=True)\n",
    "\n",
    "    master_zip_path = os.path.join(DATA_DIR, f'{COMPETITION_NAME}.zip')\n",
    "    with zipfile.ZipFile(master_zip_path, 'r') as z:\n",
    "        z.extractall(DATA_DIR)\n",
    "    for item in ['train.csv.zip', 'test.csv.zip', 'features.csv.zip']:\n",
    "        with zipfile.ZipFile(os.path.join(DATA_DIR, item), 'r') as z:\n",
    "            z.extractall(DATA_DIR)\n",
    "    print(\"Data successfully downloaded and unzipped to DBFS.\")\n",
    "else:\n",
    "    print(\"Raw data already exists in DBFS. Skipping download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d79a3e5-50a6-4cec-bc01-4027398076b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# LOAD, MERGE, AND PROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dd2c0f0-56c2-4752-81f6-2764738a6eeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load raw data from DBFS\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n",
    "features_df = pd.read_csv(os.path.join(DATA_DIR, 'features.csv'))\n",
    "stores_df = pd.read_csv(os.path.join(DATA_DIR, 'stores.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d7c7225-b7cc-4f3b-ba3b-9fc761c3cc26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The Training Set (Feb 2010 - Jan 2012)\n",
    "Action: You will train your forecasting model only on this 2-year chunk of data.\n",
    "\n",
    "The Validation Set (Feb 2012 - Oct 2012)\n",
    "\n",
    "Purpose: To give your model a \"practice exam\" and get a reliable score.\n",
    "\n",
    "Action: After training on the training set, you will use the model to forecast the 39 weeks of the validation set. You then compare your forecast to the actual sales in this period. The error you calculate here is your best estimate of your future Kaggle leaderboard score.\n",
    "\n",
    "The Competition Test Set (The real test.csv data)\n",
    "\n",
    "Purpose: To generate your final submission file for Kaggle.\n",
    "\n",
    "Action: Once you are happy with your model's performance on the validation set, you do one final step: You retrain your model on the entire original train.csv data (all 143 weeks). By using all the data available, you create the most powerful version of your model. You then use this final, fully-trained model to predict the sales for the competition test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc69b12f-3727-42be-a974-ce6b5e87fc40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad710de4-d8fc-4c87-b3dc-60ddb56862d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bca2ec35-c501-4d5d-aae2-af3b3be84d6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e35d2c7f-16d0-418e-99ec-c236fba5b58e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Merge data\n",
    "df = train_df.merge(features_df, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
    "df = df.merge(stores_df, on='Store', how='left')\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d49851b-1537-4597-9586-6a2def10ba86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 2. Split the Training Data into Train and Validation Sets ---\n",
    "from datetime import timedelta\n",
    "\n",
    "# The competition test set is 39 weeks long.\n",
    "# We will create a validation set of the same length from the end of our training data.\n",
    "validation_length_weeks = 39\n",
    "\n",
    "# Find the last date in the training data\n",
    "last_train_date = df['Date'].max()\n",
    "\n",
    "# Calculate the split date\n",
    "split_date = last_train_date - timedelta(weeks=validation_length_weeks)\n",
    "\n",
    "# Split the data\n",
    "train_data = df[df['Date'] <= split_date].copy()\n",
    "validation_data = df[df['Date'] > split_date].copy()\n",
    "\n",
    "print(f\"Data split into training and validation sets at: {split_date.date()}\")\n",
    "print(f\"Training set shape: {train_data.shape}\")\n",
    "print(f\"Validation set shape: {validation_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27cd1041-b4be-4522-8cc1-f2642bbe8fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b292ea11-31cf-48e4-b4e4-249cf3ea724f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PROCESSED_DIR = '/dbfs/FileStore/walmart_project/data/processed'\n",
    "if not os.path.exists(PROCESSED_DIR):\n",
    "    os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "from src.preprocessing import advanced_feature_engineering\n",
    "\n",
    "train_processed = advanced_feature_engineering(train_data)\n",
    "train_processed.to_csv(os.path.join(PROCESSED_DIR, 'train_final.csv'), index=False)\n",
    "\n",
    "\n",
    "validation_processed = advanced_feature_engineering(validation_data)\n",
    "validation_processed.to_csv(os.path.join(PROCESSED_DIR, 'validation_final.csv'), index=False)\n",
    "\n",
    "raw_test_data = test_df.merge(features_df, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
    "raw_test_data = raw_test_data.merge(stores_df, on='Store', how='left')\n",
    "test_processed = advanced_feature_engineering(raw_test_data)\n",
    "test_processed.to_csv(os.path.join(PROCESSED_DIR, 'test_final.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3ba48a5-3030-4aca-a30c-81be94436486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rolling feature columns\n",
    "# Reason: These missing values are expected. They appear at the beginning of each time series (for each store/department) because there isn't enough historical data to calculate the rolling window. For example, sales_roll_mean_4 is empty for the first three weeks of data for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57108599-091c-4876-a37a-c21ad1c0a9ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#checking seasonality\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf, month_plot, quarter_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cca23b87-16ec-4602-9b76-242e0d7f0b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "decomposition = seasonal_decompose(train_data['Weekly_Sales'], model = 'additive', period= 52)\n",
    "fig = decomposition.plot()\n",
    "fig.set_size_inches(14, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43c98190-5c72-43b3-b989-92d3fed953a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "decomposition = seasonal_decompose(train_data['Weekly_Sales'], model = 'add', period= 12)\n",
    "fig = decomposition.plot()\n",
    "fig.set_size_inches(14, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2b2a4a3-032c-4f4c-9cca-41774a2d810a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For Weekly_Sales data, autocorrelation tells how much this week's sales are influenced by previous weeks' sales.\n",
    "\n",
    "Identifying Momentum: A significant correlation at lag 1 means that high sales one week are likely followed by high sales the next week (and vice versa). This is a strong indicator of sales trends or momentum.\n",
    "\n",
    "Finding Seasonality: A significant correlation at lag 52 is a dead giveaway for yearly seasonality. It means the sales in a given week are strongly correlated with the sales from the same week last year. This is a critical pattern for your forecasting model to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5d14536-aaad-48c7-8226-cae257785559",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " Strong Trend / Momentum (The First Few Lags)\n",
    "\n",
    "What you see: The first few bars (lags 1, 2, 3, etc.) are very tall and extend far beyond the light blue shaded area.\n",
    "\n",
    "What it means: This indicates that the sales in any given week are highly correlated with the sales from the past few weeks. In simple terms, if sales were high last week, they are likely to be high this week. This shows a strong, positive short-term trend or \"momentum\" in the sales data.\n",
    "\n",
    "2. Clear Yearly Seasonality (The Spike at Lag 52)\n",
    "\n",
    "What you see: Look all the way to the right of the plot. There is a very clear, significant spike at lag 52.\n",
    "\n",
    "What it means: This is the most important finding. It tells you that the sales in a given week are strongly and positively correlated with the sales from the same week last year (since there are 52 weeks in a year). For example, sales during the week of Christmas this year are very similar to sales during the week of Christmas last year. This is a classic sign of yearly seasonality.\n",
    "\n",
    "What This Means for Your Project\n",
    "This plot gives you a clear roadmap for how to build your forecasting model:\n",
    "\n",
    "Because of the short-term momentum, your model needs to look at recent past values. This is the \"AR\" (AutoRegressive) part of a model like ARIMA.\n",
    "\n",
    "Because of the yearly seasonality, your model must account for this repeating annual pattern. This is the \"S\" (Seasonal) part of a model like SARIMA.\n",
    "\n",
    "Based on this plot, a SARIMA (Seasonal AutoRegressive Integrated Moving Average) model would be an excellent choice for forecasting the sales of this specific store and department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de2e3ba4-ed1e-46b4-a12c-05aa46000625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "store_id_to_analyze = 13\n",
    "dept_id_to_analyze = 7\n",
    "\n",
    "# Filter for the specific store and department\n",
    "ts_df = train_data[(train_data['Store'] == store_id_to_analyze) & (train_data['Dept'] == dept_id_to_analyze)].copy()\n",
    "\n",
    "ts_df = ts_df.set_index('Date')\n",
    "\n",
    "# Now, select the 'Weekly_Sales' column (which now has a DatetimeIndex)\n",
    "ts_data = ts_df['Weekly_Sales']\n",
    "\n",
    "# Now you can safely use .asfreq()\n",
    "ts_data = ts_data.asfreq('W-FRI')\n",
    "\n",
    "# Fill any potential missing weeks\n",
    "ts_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Check if the series is empty before plotting\n",
    "if ts_data.empty:\n",
    "    print(f\"No data available for Store {store_id_to_analyze}, Dept {dept_id_to_analyze} in the training set.\")\n",
    "else:\n",
    "    # --- Plotting Code (unchanged) ---\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    plot_acf(ts_data, lags=60, ax=ax)\n",
    "\n",
    "    plt.title(f'Autocorrelation Function (ACF) for Store {store_id_to_analyze}, Dept {dept_id_to_analyze}', fontsize=16)\n",
    "    plt.xlabel('Lag (Number of Weeks)', fontsize=12)\n",
    "    plt.ylabel('Autocorrelation', fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21b12d84-2cb9-4bf8-a93b-3919716393dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "store_id_to_analyze = 4\n",
    "dept_id_to_analyze = 1\n",
    "\n",
    "# Filter for the specific store and department\n",
    "ts_df = train_data[(train_data['Store'] == store_id_to_analyze) & (train_data['Dept'] == dept_id_to_analyze)].copy()\n",
    "\n",
    "# Set the 'Date' column as the index\n",
    "ts_df = ts_df.set_index('Date')\n",
    "\n",
    "# Select the 'Weekly_Sales' column\n",
    "ts_data = ts_df['Weekly_Sales']\n",
    "\n",
    "# Ensure a consistent weekly frequency\n",
    "ts_data = ts_data.asfreq('W-FRI')\n",
    "\n",
    "# Fill any potential missing weeks\n",
    "ts_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# --- FIX IS HERE ---\n",
    "# Request a number of lags that is less than half the sample size (104 / 2 = 52)\n",
    "# 50 is a safe and informative number.\n",
    "if not ts_data.empty and len(ts_data) > 2 * 50:\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    plot_pacf(ts_data, lags=50, ax=ax) # Changed from 60 to 50\n",
    "\n",
    "    plt.title(f'Partial Autocorrelation Function (PACF) for Store {store_id_to_analyze}, Dept {dept_id_to_analyze}', fontsize=16)\n",
    "    plt.xlabel('Lag (Number of Weeks)', fontsize=12)\n",
    "    plt.ylabel('Partial Autocorrelation', fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Not enough data to plot PACF for Store {store_id_to_analyze}, Dept {dept_id_to_analyze}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab9bb411-9e04-47d3-87c5-cca225f54268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Significant Spike at Lag 1\n",
    "\n",
    "What you see: The bar at Lag 1 is very tall and positive, extending far beyond the light blue shaded area.\n",
    "\n",
    "What it means: This shows a strong, direct relationship between this week's sales and last week's sales. After accounting for all other historical data, last week's sales figure is still the single most important predictor for this week. This is a clear signal for the \"AR\" (AutoRegressive) part of your model.\n",
    "\n",
    "2. Significant Spike at Lag 5\n",
    "\n",
    "What you see: There is a significant negative spike at Lag 5.\n",
    "\n",
    "What it means: This is interesting. It suggests that after accounting for the sales of the last four weeks, the sales from five weeks ago have a direct negative correlation with this week's sales. This could be due to a specific sales cycle, like a big promotion every month that pulls sales forward, leading to a dip later on.\n",
    "\n",
    "3. Significant Spikes Around Lag 52\n",
    "\n",
    "What you see: There are a couple of significant spikes near Lag 52 (one positive, one negative).\n",
    "\n",
    "What it means: This confirms the yearly seasonality we saw in the ACF plot. It tells you that even after accounting for all the sales in between, the sales from the same time last year still have a direct, significant impact on this week's sales.\n",
    "\n",
    "What This Means for Your Project\n",
    "This PACF plot, combined with the ACF plot, gives you a strong starting point for building a SARIMA model:\n",
    "\n",
    "The 'p' (AR) Parameter: The sharp cutoff after lag 1 in the PACF plot suggests that a p value of 1 is a good starting point for your model.\n",
    "\n",
    "The 'P' (Seasonal AR) Parameter: The significant spikes around lag 52 suggest you need a seasonal AR component. A P value of 1 would be a reasonable place to start.\n",
    "\n",
    "In summary, this plot tells you that to predict sales for this department, you should primarily look at last week's sales and the sales from this time last year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fd63be4-96a6-4075-9d77-5c39e644809b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data = train_data.set_index('Date')\n",
    "month_plot(train_data['Weekly_Sales'].resample('M').mean(), ylabel='Weekly_Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3d3b52f-4d9b-40d0-a1e1-bdf5cda0d1b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "quarter_plot(train_data['Weekly_Sales'].resample('Q').mean(), ylabel='Weekly_Sales')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_initial_data_exploration",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
