# Walmart-Recruiting---Store-Sales-Forecasting
Kaggle competition as Final project for ML

* **Elene-ს ექსპერიმენტები:**
    * **DAGSHUB-ის ბმული:** [https://dagshub.com/enelene/Walmart-Recruiting---Store-Sales-Forecasting](https://dagshub.com/enelene/Walmart-Recruiting---Store-Sales-Forecasting)
    * **WANDB-ის ბმული:** [https://wandb.ai/egabe21-free-university-of-tbilisi-/Walmart-Sales-Forecasting-DL?nw=nwuseregabe21](https://wandb.ai/egabe21-free-university-of-tbilisi-/Walmart-Sales-Forecasting-DL?nw=nwuseregabe21)

### **ფაილი 1: `00_initial_data_exploration.ipynb`**

* **დანიშნულება:** ეს არის პროექტის საწყისი სკრიპტი. მისი მიზანია, ერთჯერადად გაეშვას, რათა Kaggle-დან გადმოწეროს საწყისი მონაცემები, `src/preprocessing.py` ფაილში არსებული ლოგიკის გამოყენებით შეასრულოს კომპლექსური feature engineering და შეინახოს საბოლოო, გასუფთავებული `train_processed_final.csv` და `test_processed_final.csv` ფაილები. ეს უზრუნველყოფს, რომ ყველა შემდგომი მოდელების ნოუთბუქი ზუსტად ერთი და იმავე, სუფთა მონაცემთა წყაროდან იწყებდეს მუშაობას.
* **საკვანძო მომენტები:**
    * **მონაცემთა გაერთიანება:** `stores.csv` და `features.csv` ფაილები გაერთიანდა `train.csv` და `test.csv` ფაილებთან `left` merge-ის გამოყენებით. ამით გაყიდვების მონაცემები გამდიდრდა კონტექსტუალური ინფორმაციით.
    * **საბოლოო შედეგი:** მთავარი შედეგია საბოლოო, დამუშავებული CSV ფაილების შექმნა `data/processed` დირექტორიაში, რომელსაც ყველა სხვა ნოუთბუქი გამოიყენებს.

### **ფაილი 2: `01_advanced_data_exploration.py`**

* **დანიშნულება:** დამუშავებული მონაცემების ვიზუალური კვლევა, რათა მივიღოთ მნიშვნელოვანი ინფორმაცია (insights). ეს ნოუთბუქი განკუთვნილია ანალიზისთვის.
* **საკვანძო მომენტები:**
    * **Heatmap:** ეს ვიზუალიზაცია ადასტურებს ძლიერი წლიური სეზონურობის არსებობას, რადგან გვიჩვენებს, რომ გაყიდვების დინამიკა (მაგ., მაღალი მაჩვენებელი ნოემბერ-დეკემბერში, დაბალი კი იანვარ-თებერვალში) თანმიმდევრულია წლების განმავლობაში.
    ![image](https://github.com/user-attachments/assets/91d0743a-6243-44a5-9de9-96ecac52497e)
    * ვინაიდან ტრენდი არ არის ბრტყელი, მონაცემები არასტაციონარულია (რაც მნიშვნელოვანია ARIMA მოდელისთვის). სეზონურობის გრაფიკი გვიჩვენებს, რომ SARIMA მოდელისთვის აუცილებელია სეზონური სხვაობის (`D=1`) და `m=52` პერიოდის გამოყენება. Residuals გრაფიკი კი წარმოადგენს შემთხვევით ხმაურს, რომელიც ტრენდისა და სეზონურობის მოკლების შემდეგ რჩება.
    * **კორელაციის მატრიცა:** ეს კრიტიკულად მნიშვნელოვანი გრაფიკია. ის გვიჩვენებს, რომ არ არსებობს linear კავშირი ისეთ მახასიათებლებს შორის, როგორიცაა `Temperature` ან `CPI`, და `Weekly_Sales`-ს შორის. ეს ასაბუთებს, თუ რატომ იქნებოდა მარტივი Linear Regression მოდელი წარუმატებელი და რატომ დაგვჭირდა უფრო კომპლექსური მოდელები, როგორიცაა LightGBM ან Deep Learning.
    ![image](https://github.com/user-attachments/assets/2ff41d4f-593c-4d3f-909a-539267f72f3d)
    * **დეპარტამენტების Boxplot:** ეს გრაფიკი საუკეთესო ვიზუალური დასაბუთებაა `groupby(['Store', 'Dept'])` ლოგიკისთვის. ის ნათლად გვიჩვენებს, რომ სხვადასხვა დეპარტამენტს გაყიდვების რადიკალურად განსხვავებული მოცულობა და ცვალებადობა აქვს, რაც ამტკიცებს, რომ ისინი დამოუკიდებელ ერთეულებად უნდა განვიხილოთ.
    ![image](https://github.com/user-attachments/assets/268c4225-f55e-4216-b97e-e3ef4e703335)

### **ფაილი 3: `model_experiment_LightGBM.py`**

* **დანიშნულება:** გლობალური LightGBM მოდელის გაწვრთნა, დაკონფიგურირება და შეფასება. ეს წარმოადგენს პროგნოზირების "feature-based" მიდგომას.
* **საკვანძო მომენტები:**
    * **გლობალური მოდელი:** ერთიანი მოდელის გაწვრთნა მთლიან მონაცემთა ბაზაზე. მოდელი ერთდროულად სწავლობს 3,000-ზე მეტი დროითი row-დან, რაც მას საშუალებას აძლევს, აღმოაჩინოს გლობალური პატერნები (მაგ., "Super Bowl-ის დროს ყველა ელექტრონიკის განყოფილებაში იზრდება გაყიდვები").
    * **TimeSeriesSplit:** დროითი მწკრივების კროს-ვალიდაციის კორექტული მეთოდი, რომელიც უზრუნველყოფს, რომ მოდელი ყოველთვის წარსულ მონაცემებზე იწვრთნებოდეს და მომავალზე მოწმდებოდეს, რაც თავიდან გვაცილებს data leakage-ს.
    * **ჰიპერპარამეტრების ოპტიმიზაცია Optuna-თი:** ოპტიმიზაციის ბიბლიოთეკის გამოყენება მოდელის საუკეთესო პარამეტრების ავტომატურად საპოვნელად.
    * **Categorical მახასიათებლების მართვა:** LightGBM-ს პირდაპირ მიეწოდა ინფორმაცია Categorical სვეტების შესახებ, რაც უფრო ეფექტურია, ვიდრე one-hot encoding.
    * **MLflow Pipeline:** შეიქმნა `pyfunc` pipeline, რომელიც აერთიანებს მონაცემთა დამუშავების ფუნქციას და გაწვრთნილ მოდელს.

### **ფაილი 4: `model_experiment_ARIMA.py`**

* **დანიშნულება:** ლოკალური SARIMAX მოდელის train, tune და evaluate საუკეთესო კანდიდატების ნიმუშზე. ეს წარმოადგენს "classical statistical" მიდგომას.
* **საკვანძო მომენტები:**
    * **ლოკალური მოდელი:** ეს ნოუთბუქი LightGBM-ის საპირისპირო მიდგომას იყენებს. ის აგებს ცალკეულ, სპეციალიზებულ მოდელს თითოეული `Store-Dept` წყვილისთვის.
    * **კანდიდატების შერჩევა:** შემთხვევითი სერიის არჩევის ნაცვლად, გამოყენებულია მონაცემებზე დაფუძნებული მიდგომა ყველაზე სტაბილური და სრული ისტორიის მქონე time series საპოვნელად.
    * **ორ-ეტაპიანი ავტომატური მოდელირება:**
        1.  **მახასიათებლების შერჩევა:** `RandomForest` მოდელის გამოყენებით, თითოეული სერიისთვის ავტომატურად შეირჩა 6 ყველაზე მნიშვნელოვანი გარე მახასიათებელი.
        2.  **მართვადი `auto_arima`:** შემდეგ `auto_arima`-ს გადაეცა მხოლოდ ეს 6 საუკეთესო მახასიათებელი, რამაც გაუმარტივა და უფრო სტაბილური გახადა საუკეთესო დროითი პარამეტრების (`p,d,q,P,D,Q`) პოვნის პროცესი.
    * **"X" SARIMAX-ში:** ჩვენ გამოვიყენეთ SARIMA**X**, სადაც "X" აღნიშნავს **eXogenous** (გარე) ცვლადებს. ეს ნიშნავს, რომ კლასიკური ARIMA მოდელი გავაძლიერეთ პირველ ეტაპზე შერჩეული გარე მახასიათებლებით.

---

### **ფაილები 5 & 6: `model_experiment_NBEATS.py` & `model_experiment_TFT_darts.py`**

* **დანიშნულება:** სიღრმული სწავლების გადაწყვეტილებების კვლევა `darts` ბიბლიოთეკის გამოყენებით, რომელიც უფრო სტაბილური აღმოჩნდა ჩვენს გარემოში.
* **საკვანძო მომენტები:**
    * **ახალ ბიბლიოთეკაზე გადასვლა (`darts`):** `pytorch-forecasting`-თან დაკავშირებული გარემოს ვერსიების მუდმივი და გადაუჭრელი კონფლიქტების გამო, პროექტის დასრულების უზრუნველსაყოფად გადავედით უფრო სტაბილურ `darts` ბიბლიოთეკაზე.
    * **გლობალური vs. ლოკალური სიღრმულ სწავლებაში:** N-BEATS მოდელი გაიწვრთნა როგორც ლოკალური მოდელი (ერთ სერიაზე) და აჩვენა ცუდი შედეგი. TFT მოდელი კი გაიწვრთნა როგორც გლობალური მოდელი, ყველა სერიაზე ერთად.
    * **`TimeSeries` ობიექტი:** `darts` იყენებს სპეციალურ `TimeSeries` ობიექტს, რომელიც აერთიანებს მონაცემებსა და მათ დროით ღერძს.
    * **TFT-ის სიმძლავრე:** Temporal Fusion Transformer-ი ყველაზე მოწინავე მოდელია ჩვენს პროექტში. ის LightGBM-ის მსგავსად გლობალურია, მაგრამ მისი შიდა "ყურადღების მექანიზმი" (`attention mechanism`) სპეციალურად შექმნილია დროის გასაგებად და ავტომატურად სწავლობს, თუ რომელი მახასიათებლები და წარსულის მომენტებია ყველაზე მნიშვნელოვანი პროგნოზისთვის.

---

### **ფაილები 7 & 8: Inference ნოუთბუქები**

* **დანიშნულება:** Dagshub-დან საბოლოო, გაწვრთნილი მოდელის ჩატვირთვა და `submission.csv` ფაილის გენერირება Kaggle-ზე წარსადგენად.
* **საკვანძო მომენტები:**
    * **მოდელის რეესტრიდან ჩატვირთვა:** ჩვენ არ ვტვირთავდით ლოკალურ ფაილს. ვიყენებდით `mlflow.pyfunc.load_model("models:/...")`-ს.
    * **End-to-End Pipeline (LightGBM):** LightGBM-ის შემთხვევაში, ჩაიტვირთა `WalmartSalesPipeline`. ეს ობიექტი უზრუნველყოფს მთლიან პროცესს: იღებს **საწყის, დაუმუშავებელ** სატესტო მონაცემებს, შიდა ლოგიკით ასრულებს ყველა მახასიათებლის ინჟინერიას და აბრუნებს საბოლოო პროგნოზს.
    * **იტერაციული Inference (SARIMAX):** SARIMAX-ის submission-ის გენერირებისთვის, საჭირო გახდა ციკლის შექმნა, რომელიც თითოეული სატესტო მწკრივისთვის **ხელახლა წვრთნიდა** ახალ მოდელს მიუხედავად იმისა, რომ ლოკალური მოდელის მიდგომა საინტერესოა, ის **გამოთვლითად არაპრაქტიკულია** რეალურ სამყაროში გამოსაყენებლად, რადგან submission ფაილის გენერირებას ძალიან დიდი დრო დასჭირდა.


---

* **Qeto-ს ექსპერიმენტები:**
    * **MLFlow-ის ბმული:** [https://dagshub.com/qetibakh/Final.mlflow/#/compare-experiments/s?experiments=%5B%220%22%2C%221%22%2C%222%22%2C%223%22%5D&searchFilter=&orderByKey=attributes.start_time&orderByAsc=false&startTime=ALL&lifecycleFilter=Active&modelVersionFilter=All+Runs&datasetsFilter=W10%3D]

### **მონაცემების გასუფთავება**

* train.csv, test.csv, stores.csv და features.csv მონაცემების გაერთიანება ერთიან მონაცემთა ნაკრებში.
* 'Date' სვეტების გადაკეთება datetime ობიექტებად.
* 'Date' სვეტიდან 'year', 'month' და 'dayofweek' ინფორმაციის ამოღება.
* MarkDown სვეტებში არსებული გამოტოვებული მნიშვნელობების შევსება საშუალო და მედიანური მნიშვნელობებით.
* MarkDown ფუნქციებზე და მათ საშუალო-იმპუტირებულ ანალოგებზე ლოგარითმული, Box-Cox და Yeo-Johnson ტრანსფორმაციების გამოყენება გაუმჯობესებული განაწილებისთვის.
* მაღალ კორელაციაში მყოფი ფუნქციების (0.95-ზე მაღალი კორელაცია) ამოღება მულტიკოლინეარობის შესამცირებლად, უპირატესობა მიენიჭა 'Target' ცვლადთან უფრო ძლიერი კორელაციის მქონე ფუნქციებს.
* გასუფთავებული მონაცემთა ნაკრების შენახვა Clean_training.csv ფაილად.
* მონაცემთა ნაკრების დაყოფა სასწავლო, ვალიდაციისა და ტესტირების ნაწილებად თარიღის მიხედვით (უახლესი მონაცემები test-ში).

### **მოდელის ექსპერიმენტები**

* ხეებზე დაფუძნებული რეგრესიის ექვსი მოდელის შეფასება: LightGBM, XGBoost, CatBoost, HistGradientBoostingRegressor, ExtraTreesRegressor და RandomForestRegressor.
* შეფასების ძირითადი მეტრიკები იყო: ფესვის საშუალო კვადრატული შეცდომა (RMSE), შეწონილი საშუალო აბსოლუტური შეცდომა (WMAE) და R-კვადრატი (R^2).
* ExtraTreesRegressor-მა აჩვენა საუკეთესო შედეგი ყველაზე დაბალი ვალიდაციის RMSE (7694.90) და ტესტის RMSE (6676.52).

### **რა არის გასაკეთებელი:**
* ჰიპერპარამეტრების დამატება
* აუთლაიერების ამოგდება
* უკეთესი featureების გამოყვანა https://www.kaggle.com/code/maxdiazbattan/wallmart-sales-top-3-eda-feature-engineering#-3.1-|-Sales-analysis
* featureბის შერჩევა
